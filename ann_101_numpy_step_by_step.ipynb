{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ann_101_numpy.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6p8U6Qd15-y1"
      },
      "source": [
        "## Import Python Librairies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nwgSSb8y5-y6",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KMyZPu-J5-zP"
      },
      "source": [
        "## Prepare the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ItA87Kav5-zX",
        "colab": {}
      },
      "source": [
        "X_train = np.array([1.0,])\n",
        "Y_train = np.array([-2.0,])"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1kvlnc_X5-0T"
      },
      "source": [
        "## Build the artificial neural-network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gd_3obZd5-0Z",
        "colab": {}
      },
      "source": [
        "ANN_ARCHITECTURE = [\n",
        "    {\"input_dim\": 1, \"output_dim\": 2, \"activation\": \"relu\"},\n",
        "    {\"input_dim\": 2, \"output_dim\": 2, \"activation\": \"relu\"},\n",
        "    {\"input_dim\": 2, \"output_dim\": 1, \"activation\": \"none\"},\n",
        "]"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hV0v7-VNkcuG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "PSEUDO_RANDOM_PARAM_VALUES = {\n",
        "    'W1': np.array([[ 0.01],\n",
        "                    [-0.03]]),\n",
        "    'b1': np.array([[ 0.02],\n",
        "                    [-0.04]]),\n",
        "    'W2': np.array([[ 0.05, -0.06 ],\n",
        "                    [-0.07,  0.08]]),\n",
        "    'b2': np.array([[ 0.09],\n",
        "                    [-0.10]]),\n",
        "    'W3': np.array([[-0.11, -0.12]]),\n",
        "    'b3': np.array([[-0.13]])\n",
        "}"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qhYYy64W5-06",
        "colab": {}
      },
      "source": [
        "def relu(Z):\n",
        "    return np.maximum(0,Z)\n",
        "\n",
        "def relu_backward(dA, Z):\n",
        "    dZ = np.array(dA, copy = True)\n",
        "    dZ[Z <= 0] = 0;\n",
        "    return dZ;"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "02RwcYdi5-1D"
      },
      "source": [
        "### Single layer forward propagation step"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0jqwnYlC5-1H"
      },
      "source": [
        "$$\\boldsymbol{Z}^{[l]} = \\boldsymbol{W}^{[l]} \\cdot \\boldsymbol{A}^{[l-1]} + \\boldsymbol{b}^{[l]}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1mzTAqQN5-1K"
      },
      "source": [
        "$$\\boldsymbol{A}^{[l]} = g^{[l]}(\\boldsymbol{Z}^{[l]})$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Bjg-hId65-1L",
        "colab": {}
      },
      "source": [
        "def single_layer_forward_propagation(A_prev, W_curr, b_curr, activation):\n",
        "    \n",
        "    # calculation of the input value for the activation function\n",
        "    Z_curr = np.dot(W_curr, A_prev) + b_curr    \n",
        "    \n",
        "    # selection of activation function\n",
        "    if activation == \"none\":\n",
        "        return Z_curr, Z_curr\n",
        "    elif activation == \"relu\":\n",
        "        activation_func = relu\n",
        "    else:\n",
        "        raise Exception('Non-supported activation function')\n",
        "        \n",
        "    # return of calculated activation A and the intermediate Z matrix\n",
        "    return activation_func(Z_curr), Z_curr"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8_GTvyJ45-1Y",
        "colab": {}
      },
      "source": [
        "def full_forward_propagation(X, params_values, ann_architecture):\n",
        "    # creating a temporary memory to store the information needed for a backward step\n",
        "    memory = {}\n",
        "    # X vector is the activation for layer 0 \n",
        "    A_curr = X\n",
        "    \n",
        "    # iteration over network layers\n",
        "    for idx, layer in enumerate(ann_architecture):\n",
        "        # we number network layers starting from 1\n",
        "        layer_idx = idx + 1\n",
        "        # transfer the activation from the previous iteration\n",
        "        A_prev = A_curr\n",
        "        \n",
        "        # extraction of the activation function for the current layer\n",
        "        activ_function_curr = layer[\"activation\"]\n",
        "        \n",
        "        # extraction of W for the current layer\n",
        "        W_curr = params_values[\"W\" + str(layer_idx)]\n",
        "        # extraction of b for the current layer\n",
        "        b_curr = params_values[\"b\" + str(layer_idx)]\n",
        "        # calculation of activation for the current layer\n",
        "        A_curr, Z_curr = single_layer_forward_propagation(A_prev, W_curr, b_curr, activ_function_curr)\n",
        "        \n",
        "        # saving calculated values in the memory\n",
        "        memory[\"A\" + str(idx)] = A_prev\n",
        "        memory[\"Z\" + str(layer_idx)] = Z_curr\n",
        "    \n",
        "    # return of prediction vector and a dictionary containing intermediate values\n",
        "    return A_curr, memory"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MLV7JlOD5-1j",
        "colab": {}
      },
      "source": [
        "def get_cost_value(Ŷ, Y):\n",
        "    # this cost function works for 1-dimension only\n",
        "    # to do: use a quadratic function instead\n",
        "    cost = Ŷ - Y\n",
        "    return np.squeeze(cost)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ead5c-ze5-12",
        "colab": {}
      },
      "source": [
        "def single_layer_backward_propagation(dA_curr, W_curr, b_curr, Z_curr, A_prev, activation, layer, debug=False):\n",
        "    \n",
        "    # selection of activation function\n",
        "    if activation == \"none\":\n",
        "        backward_activation_func = relu_backward\n",
        "        \n",
        "        dZ_curr = dA_curr\n",
        "\n",
        "    else:\n",
        "        if activation == \"relu\":\n",
        "            backward_activation_func = relu_backward\n",
        "        else:\n",
        "            raise Exception('Non-supported activation function')\n",
        "        \n",
        "        # calculation of the activation function derivative\n",
        "        dZ_curr = backward_activation_func(dA_curr, Z_curr)\n",
        "        if debug:\n",
        "            print('Step_4: layer',layer,'dZ=', dZ_curr.tolist())\n",
        "        \n",
        "    # derivative of the matrix W\n",
        "    dW_curr = np.dot(dZ_curr, A_prev.T)\n",
        "    if debug:\n",
        "        # tolist() allows printing a numpy array on a single debug line\n",
        "        print('Step_4: layer',layer,'dW=dZ.A_prev.T=', dZ_curr.tolist(), '.', A_prev.T.tolist())\n",
        "        print('                dW=', dW_curr.tolist())\n",
        "    \n",
        "    # derivative of the vector b\n",
        "    db_curr = np.sum(dZ_curr, axis=1, keepdims=True)\n",
        "    if debug:\n",
        "        print('Step_4: layer',layer,'db=', db_curr.tolist())\n",
        "    \n",
        "    # derivative of the matrix A_prev\n",
        "    dA_prev = np.dot(W_curr.T, dZ_curr)\n",
        "    if debug:\n",
        "        print('Step_4: layer',layer,'dA_prev=W.T.dZ=', W_curr.T.tolist(), '.', dZ_curr.tolist())\n",
        "        print('                dA_prev=', dA_prev.tolist())\n",
        "        \n",
        "    return dA_prev, dW_curr, db_curr"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dPJsDoxg5-2G",
        "colab": {}
      },
      "source": [
        "def full_backward_propagation(Ŷ, cost, memory, params_values, ann_architecture, debug=False):\n",
        "    \n",
        "    grads_values = {}\n",
        "    \n",
        "    # number of examples\n",
        "    m = Ŷ.shape[1]\n",
        "    \n",
        "    # initiation of gradient descent algorithm\n",
        "    dA_prev = cost.reshape(Ŷ.shape)\n",
        "    \n",
        "    for layer_idx_prev, layer in reversed(list(enumerate(ann_architecture))):\n",
        "        \n",
        "        # we number network layers from 1\n",
        "        layer_idx_curr = layer_idx_prev + 1\n",
        "        \n",
        "        # extraction of the activation function for the current layer\n",
        "        activ_function_curr = layer[\"activation\"]    \n",
        "        \n",
        "        dA_curr = dA_prev\n",
        "        \n",
        "        A_prev = memory[\"A\" + str(layer_idx_prev)]\n",
        "        Z_curr = memory[\"Z\" + str(layer_idx_curr)]\n",
        "        \n",
        "        W_curr = params_values[\"W\" + str(layer_idx_curr)]\n",
        "        b_curr = params_values[\"b\" + str(layer_idx_curr)]\n",
        "        \n",
        "        dA_prev, dW_curr, db_curr = single_layer_backward_propagation(\n",
        "            dA_curr, W_curr, b_curr, Z_curr, A_prev, activ_function_curr, layer_idx_curr, debug)\n",
        "        \n",
        "        grads_values[\"dW\" + str(layer_idx_curr)] = dW_curr\n",
        "        grads_values[\"db\" + str(layer_idx_curr)] = db_curr\n",
        "    \n",
        "    return grads_values"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sBDEJsKs5-2X",
        "colab": {}
      },
      "source": [
        "def update(params_values, grads_values, ann_architecture, learning_rate, m):\n",
        "\n",
        "    # iteration over network layers\n",
        "    for layer_idx, layer in enumerate(ann_architecture, 1):\n",
        "        params_values[\"W\" + str(layer_idx)] -= learning_rate * grads_values[\"dW\" + str(layer_idx)] / m     \n",
        "        params_values[\"b\" + str(layer_idx)] -= learning_rate * grads_values[\"db\" + str(layer_idx)] / m        \n",
        "        \n",
        "    return params_values;"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uEpJ0Bfp5-2m",
        "colab": {}
      },
      "source": [
        "def train(X, Y, ann_architecture, params_values, learning_rate, debug=False, callback=None):\n",
        "    # initiation of neural net parameters\n",
        "    \n",
        "    # initiation of lists storing the history \n",
        "    # of metrics calculated during the learning process \n",
        "    cost_history = []\n",
        "    \n",
        "    # performing calculations for subsequent iterations\n",
        "    Ŷ, memory = full_forward_propagation(X, params_values, ann_architecture)\n",
        "    if debug:\n",
        "        print('Step_2: memory=%s', memory)        \n",
        "        print('Step_2: Ŷ=', Ŷ)\n",
        "        \n",
        "    # calculating metrics and saving them in history (just for future information)\n",
        "    cost = get_cost_value(Ŷ, Y)\n",
        "    if debug:\n",
        "        print('Step_3: cost=%.5f' % cost)\n",
        "    cost_history.append(cost)\n",
        "       \n",
        "    # step backward - calculating gradient\n",
        "    grads_values = full_backward_propagation(Ŷ, cost, memory, params_values, ann_architecture, debug)\n",
        "    \n",
        "    #print('grads_values:',grads_values)\n",
        "    # updating model state\n",
        "    m = X.shape[0] # m is number of samples in the batch\n",
        "    params_values = update(params_values, grads_values, ann_architecture, learning_rate, m)\n",
        "    if debug:\n",
        "        print('Step_5: params_values=', params_values)\n",
        "    return params_values"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "L-WcNJGD5-22",
        "colab": {}
      },
      "source": [
        "X_train = X_train.reshape(X_train.shape[0], 1)\n",
        "Y_train = Y_train.reshape(Y_train.shape[0], 1)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3EwF5oQQ5-3T"
      },
      "source": [
        "## Train the artificial neural-network model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gLExhKBU5-3W",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d35dae9b-beb2-478a-f20d-96b00a1aa046"
      },
      "source": [
        "debug = True\n",
        "\n",
        "# Training\n",
        "ann_architecture = ANN_ARCHITECTURE\n",
        "param_values = PSEUDO_RANDOM_PARAM_VALUES.copy()\n",
        "\n",
        "if debug:\n",
        "    print('X_train:', X_train)\n",
        "    print('Y_train:', Y_train)\n",
        "    print('ann_architecture:', ANN_ARCHITECTURE)\n",
        "\n",
        "# implementation of the stochastic gradient descent\n",
        "EPOCHS = 2\n",
        "for epoch in range(EPOCHS):\n",
        "    \n",
        "    if debug:\n",
        "        print('##### EPOCH %d #####' % epoch)\n",
        "        print('Step_0: param_values:', param_values)\n",
        "    \n",
        "    samples_per_batch = 1\n",
        "    \n",
        "    for i in range(int(X_train.shape[0]/samples_per_batch)):\n",
        "        si = i * samples_per_batch\n",
        "        sj = (i + 1) * samples_per_batch\n",
        "\n",
        "        if debug:\n",
        "            print('Step_1: X_train[%d,%d]=%s' % (si, sj, X_train[si:sj]))\n",
        "\n",
        "        learning_rate = 0.01\n",
        "        \n",
        "        param_values = train(\n",
        "            np.transpose(X_train[si:sj]), \n",
        "            np.transpose(Y_train[si:sj]),\n",
        "            ann_architecture, \n",
        "            param_values, \n",
        "            learning_rate,\n",
        "            debug) "
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_train: [[1.]]\n",
            "Y_train: [[-2.]]\n",
            "ann_architecture: [{'input_dim': 1, 'output_dim': 2, 'activation': 'relu'}, {'input_dim': 2, 'output_dim': 2, 'activation': 'relu'}, {'input_dim': 2, 'output_dim': 1, 'activation': 'none'}]\n",
            "##### EPOCH 0 #####\n",
            "Step_0: param_values: {'W1': array([[ 0.01],\n",
            "       [-0.03]]), 'b1': array([[ 0.02],\n",
            "       [-0.04]]), 'W2': array([[ 0.05, -0.06],\n",
            "       [-0.07,  0.08]]), 'b2': array([[ 0.09],\n",
            "       [-0.1 ]]), 'W3': array([[-0.11, -0.12]]), 'b3': array([[-0.13]])}\n",
            "Step_1: X_train[0,1]=[[1.]]\n",
            "Step_2: memory=%s {'A0': array([[1.]]), 'Z1': array([[ 0.03],\n",
            "       [-0.07]]), 'A1': array([[0.03],\n",
            "       [0.  ]]), 'Z2': array([[ 0.0915],\n",
            "       [-0.1021]]), 'A2': array([[0.0915],\n",
            "       [0.    ]]), 'Z3': array([[-0.140065]])}\n",
            "Step_2: Ŷ= [[-0.140065]]\n",
            "Step_3: cost=1.85994\n",
            "Step_4: layer 3 dW=dZ.A_prev.T= [[1.8599350000000001]] . [[0.0915, 0.0]]\n",
            "                dW= [[0.17018405250000002, 0.0]]\n",
            "Step_4: layer 3 db= [[1.8599350000000001]]\n",
            "Step_4: layer 3 dA_prev=W.T.dZ= [[-0.11], [-0.12]] . [[1.8599350000000001]]\n",
            "                dA_prev= [[-0.20459285000000002], [-0.2231922]]\n",
            "Step_4: layer 2 dZ= [[-0.20459285000000002], [0.0]]\n",
            "Step_4: layer 2 dW=dZ.A_prev.T= [[-0.20459285000000002], [0.0]] . [[0.03, 0.0]]\n",
            "                dW= [[-0.0061377855, 0.0], [0.0, 0.0]]\n",
            "Step_4: layer 2 db= [[-0.20459285000000002], [0.0]]\n",
            "Step_4: layer 2 dA_prev=W.T.dZ= [[0.05, -0.07], [-0.06, 0.08]] . [[-0.20459285000000002], [0.0]]\n",
            "                dA_prev= [[-0.010229642500000002], [0.012275571]]\n",
            "Step_4: layer 1 dZ= [[-0.010229642500000002], [0.0]]\n",
            "Step_4: layer 1 dW=dZ.A_prev.T= [[-0.010229642500000002], [0.0]] . [[1.0]]\n",
            "                dW= [[-0.010229642500000002], [0.0]]\n",
            "Step_4: layer 1 db= [[-0.010229642500000002], [0.0]]\n",
            "Step_4: layer 1 dA_prev=W.T.dZ= [[0.01, -0.03]] . [[-0.010229642500000002], [0.0]]\n",
            "                dA_prev= [[-0.00010229642500000002]]\n",
            "Step_5: params_values= {'W1': array([[ 0.0101023],\n",
            "       [-0.03     ]]), 'b1': array([[ 0.0201023],\n",
            "       [-0.04     ]]), 'W2': array([[ 0.05006138, -0.06      ],\n",
            "       [-0.07      ,  0.08      ]]), 'b2': array([[ 0.09204593],\n",
            "       [-0.1       ]]), 'W3': array([[-0.11170184, -0.12      ]]), 'b3': array([[-0.14859935]])}\n",
            "##### EPOCH 1 #####\n",
            "Step_0: param_values: {'W1': array([[ 0.0101023],\n",
            "       [-0.03     ]]), 'b1': array([[ 0.0201023],\n",
            "       [-0.04     ]]), 'W2': array([[ 0.05006138, -0.06      ],\n",
            "       [-0.07      ,  0.08      ]]), 'b2': array([[ 0.09204593],\n",
            "       [-0.1       ]]), 'W3': array([[-0.11170184, -0.12      ]]), 'b3': array([[-0.14859935]])}\n",
            "Step_1: X_train[0,1]=[[1.]]\n",
            "Step_2: memory=%s {'A0': array([[1.]]), 'Z1': array([[ 0.03020459],\n",
            "       [-0.07      ]]), 'A1': array([[0.03020459],\n",
            "       [0.        ]]), 'Z2': array([[ 0.09355801],\n",
            "       [-0.10211432]]), 'A2': array([[0.09355801],\n",
            "       [0.        ]]), 'Z3': array([[-0.15904995]])}\n",
            "Step_2: Ŷ= [[-0.15904995]]\n",
            "Step_3: cost=1.84095\n",
            "Step_4: layer 3 dW=dZ.A_prev.T= [[1.8409500478597611]] . [[0.09355801203562028, 0.0]]\n",
            "                dW= [[0.17223562673463927, 0.0]]\n",
            "Step_4: layer 3 db= [[1.8409500478597611]]\n",
            "Step_4: layer 3 dA_prev=W.T.dZ= [[-0.111701840525], [-0.12]] . [[1.8409500478597611]]\n",
            "                dA_prev= [[-0.20563750866052216], [-0.22091400574317133]]\n",
            "Step_4: layer 2 dZ= [[-0.20563750866052216], [0.0]]\n",
            "Step_4: layer 2 dW=dZ.A_prev.T= [[-0.20563750866052216], [0.0]] . [[0.03020459285, 0.0]]\n",
            "                dW= [[-0.006211197223779421, 0.0], [0.0, 0.0]]\n",
            "Step_4: layer 2 db= [[-0.20563750866052216], [0.0]]\n",
            "Step_4: layer 2 dA_prev=W.T.dZ= [[0.050061377855000005, -0.07], [-0.06, 0.08]] . [[-0.20563750866052216], [0.0]]\n",
            "                dA_prev= [[-0.010294497022215236], [0.01233825051963133]]\n",
            "Step_4: layer 1 dZ= [[-0.010294497022215236], [0.0]]\n",
            "Step_4: layer 1 dW=dZ.A_prev.T= [[-0.010294497022215236], [0.0]] . [[1.0]]\n",
            "                dW= [[-0.010294497022215236], [0.0]]\n",
            "Step_4: layer 1 db= [[-0.010294497022215236], [0.0]]\n",
            "Step_4: layer 1 dA_prev=W.T.dZ= [[0.010102296425, -0.03]] . [[-0.010294497022215236], [0.0]]\n",
            "                dA_prev= [[-0.00010399806046469814]]\n",
            "Step_5: params_values= {'W1': array([[ 0.01020524],\n",
            "       [-0.03      ]]), 'b1': array([[ 0.02020524],\n",
            "       [-0.04      ]]), 'W2': array([[ 0.05012349, -0.06      ],\n",
            "       [-0.07      ,  0.08      ]]), 'b2': array([[ 0.0941023],\n",
            "       [-0.1      ]]), 'W3': array([[-0.1134242, -0.12     ]]), 'b3': array([[-0.16700885]])}\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}